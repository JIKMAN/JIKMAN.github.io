---
title:  "[Hadoop] 분산 처리 시스템"

categories:
  - Hadoop
tags:
  - [HDFS, hadoop]
toc: true
toc_sticky: true
---



## **분산 처리 시스템**



하나의 작업에 여러대의 machine을 두고, **MPI(Message Passing Interface)**를 사용하는 시스템입니다. 하지만, 분산처리 시스템에도 문제가 있습니다.

\- 복잡한 프로그래밍(데이터 프로세스의 sync 유지) : MPI가 프로그래밍 하기 굉장히 복잡합니다.

\- Partial failures : 수많은 컴퓨터를 사용하는 경우에 일부의 컴퓨터가 고장나는 경우 시스템이 동작하지 않습니다.



### **✔️** 하둡 분산 처리 시스템

GFS(구글파일시스템)과 MapReduce가 나오기전, 분산처리 시스템은 Message Passing Inteface가 너무 복잡하여 프로그래밍하기가 어려웠습니다. 또한, 수만대의 분산 컴퓨터들이 하나만 고장이나도 동작을 안하게 되는 문제가 있었습니다. 이 문제를 해결한 것이 바로 Hadoop 시스템입니다.



![img](https://t1.daumcdn.net/cfile/tistory/2351FD385902BB6C2D)



시스템은 반드시 partial failure에 대처가 요구됩니다. 컴포넌트의 failure(전체 시스템의 failure가 아닌)는 애플리케이션 성능 저하를 유발합니다. 이에 대한 해결책으로 구글이 생각한 것은 3개의 COPY를 두는 것이었습니다. 컴퓨터가 많이 필요하겠지만, 기본적으로 레거시한 컴퓨터를 쓰기때문에 몇대가 더 추가된다고 비용문제가 크게 발생하지는 않습니다.



![img](https://t1.daumcdn.net/cfile/tistory/2443DE3C5902C4F815)



**서버에서 가지고 있는 파일 메타데이터를 확인하여, C1|C2|C3 중에 하나의 데이터를 참고하면 됩니다.** 



하둡의 분산처리 시스템에서 제공하는 특징은 아래와 같습니다.



**1) 데이터 Recoverablility**

시스템의 컴포넌트가 fail하더라도 시스템을 통해 작업을 지속적으로 수행되어야 합니다. 즉, failure로 인해 어떠한 데이터의 손실도 발생해서는 안됩니다. 이러한 아이디어를 GFS에서 제공하였습니다.



**2) 컴포넌트 Recovery**

시스템의 컴포넌트가 fail되고 다시 recover된 경우, 시스템에 rejoin하는 것이 가능해야합니다. 이 작업은 전체 시스템의 재시작없이 수행되어야 합니다.



**3) Consistency**

job이 수행되는 동안 컴포넌트의 failure는 결과에 영향을 주지 않아야 합니다. (위 데이터 Recoverablility와 컴포넌트 Recovery로 해결이 됩니다.)



**4) Scalability**

데이터의 양이 증가하면, 각 작업의 성능이 감소합니다. (시스템은 fail되지 않음)

시스템의 resource를 증가시키면, 비례적으로 로드 capacity가 증가합니다.



![img](https://t1.daumcdn.net/cfile/tistory/2320523C5902C0C325)





### ✔️ **Hadoop의 역사**



**1) Hadoop은 1990년~2000년 사이의 Google의 연구로부터 시작**

 \- Google File System(GFS) in 2003

 \- MapReduce in 2004

 **실제 아파치 오픈소스 파운데이션에서 위의 둘을 가져다 만든 소프트웨어 프레임워크가 하둡**



**2) 기존 분산 컴퓨팅의 문제점을 해결할 수 있는 새로운 접근법**

 \- reliability와 scalability 문제를 모두 해결



**3) Core concept : 초기 데이터를 시스템에 분산하여 저장**

 \- 클러스터의 각 노드가 로컬 데이터에 대한 작업을 처리

  initial processing을 위해 네트워크를 통해 데이터가 전송되지 않음



### **✔️ Hadoop의 코어 컨셉 : Easy to Use**

<br>

**1) 애플리케이션을 High-level 코드로 작성**

 \- 개발자에게 네트워크 프로그래밍, 의존성, low-level 인프라 구조에 대한 고려가 요구되지 않음



**2) 각 노드들은 가급적 최소한의 데이터를 주고 받음**

 \- 개발자는 노드들 사이의 통신에 대한 코드 작성이 필요없음.

 \- 'Shared nothing' architecture

빅데이터를 사용하게되면 호스트간 데이터를 주고받는데 시간이 굉장히 오래 걸리게 됩니다. **호스트간 데이터 전송을 최소화하는 것이 유리**합니다.



**3) 데이터는 여러 노드에 미리 분산되어 저장**

 \- 데이터가 저장된 위치에서 연산을 수행

  availability와 reliability를 향상 시키기 위해서 데이터는 여러 개의 본제본을 두고 사용.





### **✔️** **Hadoop : Very High-Level Overview**



![img](https://t1.daumcdn.net/cfile/tistory/267B34395902C5120B)





![img](https://t1.daumcdn.net/cfile/tistory/21792A3A5902C53718)



**1)** **시스템이 데이터를 로드할 때 'block'으로 나누어짐 (기본적으로 64MB 또는 128MB 단위로 데이터를 자름)**

\- 일반적으로 64MB이고, 파일 연산시 블락단위 연산이 이루어 집니다.

**2)** **Map tasks (MapReduce 시스템의 첫번째 파트) 는 single block 단위의 작업을 처리**

\- 하둡은 MapReduce 방식으로 모든 연산 작업이 이루어집니다.

**3)** **마스터 프로그램은 분산되어 저장된 데이터의 block에 대한 Map task 작업을 각 노드에 할당**

\- 전체 데이터 셋 중 각 노드에 저장된 데이터를 이용해 병렬적으로 작업 수행





### **✔️** **Fault Tolerance**



![img](https://t1.daumcdn.net/cfile/tistory/225F74365902C6421D)



1) Node가 fail한 경우, master node는 failure를 감지하고 작업을 다른 node에 할당합니다.

2) Task를 재시작하는 것은 다른 부분에 대한 작업을 수행중인 다른 노드와의 통신을 필요로 하지 않습니다.

3) Fail된 node를 재시작하는 경우, 자동적으로 시스템에 연결되어 새로운 task를 할당합니다.

4) 특정 Node의 성능이 매우 낮은 것으로 감지되면, master node는 같은 task를 다른 node에 할당합니다.

 (Speculative Execution)





![img](https://t1.daumcdn.net/cfile/tistory/264CAD365902C75102)



![img](https://t1.daumcdn.net/cfile/tistory/221A66345902C77B38)





노드는 곧 컴퓨터입니다. **컴퓨터안에는 cpu, memory 등이 있고, 메모리상에 데이터가 블럭단위로 저장되어 cpu는 MapReduce 연산을 수행**합니다. 



출처: https://12bme.tistory.com/71?category=737765